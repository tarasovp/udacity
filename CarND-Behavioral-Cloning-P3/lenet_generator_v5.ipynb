{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/root/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.python.control_flow_ops = tf\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=str(1)\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, ConvLSTM2D, Dense, MaxPooling2D, Dropout, Flatten, Reshape, merge, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "\n",
    "# Process single image\n",
    "def proc_img(img): # input is 160x320x3\n",
    "    img = img[59:138:2, 0:-1:2, :] # select vertical region and take each second pixel to reduce image dimensions\n",
    "    img = (img / 127.5) - 1.0 # normalize colors from 0-255 to -1.0 to 1.0\n",
    "    return img # return 40x160x3 image\n",
    "\n",
    "\n",
    "from get_images import get_images\n",
    "\n",
    "datadirs=['/notebooks/udacity/new_training/map1_backward/',\n",
    "                 '/notebooks/udacity/new_training/map1_forward/',\n",
    "                 '/notebooks/udacity/new_training/map1_recovery_backward/',\n",
    "                 '/notebooks/udacity/new_training/map1_recovery_forward/',\n",
    "                 '/notebooks/udacity/new_training/map2_forward/',\n",
    "                 '/notebooks/udacity/new_training/map2_backward/',\n",
    "                 '/notebooks/udacity/new_training/map2_recovery_forward/',\n",
    "                 '/notebooks/udacity/new_training/map2_recovery_backward/',\n",
    "                   '/notebooks/udacity/new_training/map1_error_correction/',\n",
    "                   '/notebooks/udacity/new_training/map2_error_correction/'\n",
    "         ]\n",
    "\n",
    "images=get_images(datadirs,0.08)\n",
    "image_names_full, y_data_full = images.img.values, images.real.values\n",
    "\n",
    "# Random sort for data and split test and validation sets\n",
    "def newRandomTestValidationSplit(X, y):\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.01, random_state=111)\n",
    "    return X_tr, X_val, y_tr, y_val\n",
    "\n",
    "# Batch generator for training data\n",
    "def generate_image_batch_tr(names, y_data, batch_size = 32):\n",
    "    total_items = len(names)\n",
    "    curr_item = 0\n",
    "    while (True):\n",
    "        image_data = np.zeros((batch_size,40,160, 3),dtype=float)\n",
    "        steering_data = np.zeros((batch_size),dtype=float)\n",
    "        for j in range(batch_size):\n",
    "            image_name = names[curr_item]\n",
    "            image = mpimg.imread(image_name)\n",
    "            image_data[j] = proc_img(image)\n",
    "            steering_data[j] = y_data[curr_item]\n",
    "            curr_item = (curr_item+1)%total_items\n",
    "        yield image_data, steering_data\n",
    "\n",
    "# Batch generator for validation data (in this implementation same as for training data)\n",
    "generate_image_batch=generate_image_batch_tr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# Model - ideas from VG type network\n",
    "inp = Input(shape=(40,160,3))\n",
    "# First convolution is for model to determine the 'best' colorspace weights\n",
    "x = Conv2D(3, 1, 1, border_mode='same', activation='relu')(inp)\n",
    "# Reduce dimensions\n",
    "x = MaxPooling2D((2,2))(x) #20x80\n",
    "\n",
    "# First convolution layer\n",
    "x1 = Conv2D(32, 3, 3, border_mode='same', activation='relu')(x)\n",
    "x1 = Conv2D(32, 3, 3, border_mode='same', activation='relu')(x1)\n",
    "x1 = MaxPooling2D((2,2))(x1) #10x40\n",
    "x1 = Dropout(0.5)(x1)\n",
    "flat1 = Flatten()(x1) # Used for the merge before first fully connected layer\n",
    "\n",
    "# Second convolution layer\n",
    "x2 = Conv2D(64, 3, 3, border_mode='same', activation='relu')(x1)\n",
    "x2 = Conv2D(64, 3, 3, border_mode='same', activation='relu')(x2)\n",
    "x2 = MaxPooling2D((2,2))(x2) #5x20\n",
    "x2 = Dropout(0.5)(x2)\n",
    "flat2 = Flatten()(x2) # Used for the merge before first fully connected layer\n",
    "\n",
    "# Second convolution layer\n",
    "x3 = Conv2D(64, 3, 3, border_mode='same', activation='relu')(x2)\n",
    "x3 = Conv2D(64, 3, 3, border_mode='same', activation='relu')(x3)\n",
    "x3 = MaxPooling2D((2,2))(x3) #2x10\n",
    "x3 = Dropout(0.5)(x3)\n",
    "flat3 = Flatten()(x3) # Used for the merge before first fully connected layer\n",
    "\n",
    "# Merge the flattened ouputs after each convolution layer\n",
    "x4 = merge([flat1, flat2, flat3], mode='concat')\n",
    "# Fully connected layers\n",
    "x5 = Dense(512, activation='relu')(x4)\n",
    "x6 = Dense(128, activation='relu')(x5)\n",
    "x7 = Dense(16, activation='relu')(x6)\n",
    "out = Dense(1, activation='linear')(x7)\n",
    "\n",
    "model = Model(input=inp, output=out)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed widget Javascript is the wrong version.\n",
      "The installed widget Javascript is the wrong version.\n",
      "The installed widget Javascript is the wrong version.\n",
      "The installed widget Javascript is the wrong version.\n",
      "The installed widget Javascript is the wrong version.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import History,TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"model_gen5.h5\" , monitor='val_mean_squared_error', verbose=False,\n",
    "                          save_best_only=True, mode='min')\n",
    "early_stop = EarlyStopping(monitor='val_mean_squared_error',\\\n",
    "                           min_delta=0.001, patience=3,\n",
    "                            verbose=1, mode='min')\n",
    "\n",
    "# Compile, train and save\n",
    "#model.compile(optimizer=Adam(lr=FLAGS.learn_rate), loss='mse')\n",
    "\n",
    "\n",
    "model.compile(loss='mse', optimizer=Adam(lr=0.0001), metrics=['mean_squared_error'])\n",
    "\n",
    "\n",
    "#print ('Split data')\n",
    "#X_tr_names, X_val_names, y_tr, y_val = newRandomTestValidationSplit(image_names_full, y_data_full)\n",
    "\n",
    "X_tr_names, X_val_names, y_tr, y_val = train_test_split(image_names_full, y_data_full, test_size=0.02,\\\n",
    "                            random_state=0)\n",
    "X_test_names, X_val_names, y_test, y_val = train_test_split(X_val_names, y_val, test_size=0.5,\\\n",
    "                                        random_state=0)\n",
    "\n",
    "\n",
    "print ('Start training')\n",
    "# Training and validation inputs are fed from generators\n",
    "# Number of samples based on data_set size and adjusted to fit batch size\n",
    "history = model.fit_generator(generate_image_batch_tr(X_tr_names, y_tr, 64),samples_per_epoch=len(X_tr_names),\n",
    "                              nb_epoch=5,\n",
    "                              validation_data=generate_image_batch(X_val_names, y_val, 32),\n",
    "                              nb_val_samples=len(y_val),  callbacks=[checkpoint, early_stop, TQDMNotebookCallback()],\\\n",
    "                              verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.024803815377426559, 0.024803815377426559]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(generate_image_batch(X_test_names,y_test,32), val_samples=len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from generator import generator\n",
    "from keras_tqdm import TQDMNotebookCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed widget Javascript is the wrong version.\n",
      "The installed widget Javascript is the wrong version.\n",
      "/root/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training.py:1569: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n",
      "The installed widget Javascript is the wrong version.\n",
      "The installed widget Javascript is the wrong version.\n",
      "The installed widget Javascript is the wrong version.\n",
      "The installed widget Javascript is the wrong version.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import History,TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"model_gen5-mygen.h5\" , monitor='val_mean_squared_error', verbose=False,\n",
    "                          save_best_only=True, mode='min')\n",
    "early_stop = EarlyStopping(monitor='val_mean_squared_error',\\\n",
    "                           min_delta=0.001, patience=3,\n",
    "                            verbose=1, mode='min')\n",
    "\n",
    "# Compile, train and save\n",
    "#model.compile(optimizer=Adam(lr=FLAGS.learn_rate), loss='mse')\n",
    "\n",
    "\n",
    "model.compile(loss='mse', optimizer=Adam(lr=0.0001), metrics=['mean_squared_error'])\n",
    "\n",
    "\n",
    "#print ('Split data')\n",
    "#X_tr_names, X_val_names, y_tr, y_val = newRandomTestValidationSplit(image_names_full, y_data_full)\n",
    "\n",
    "X_tr_names, X_val_names, y_tr, y_val = train_test_split(image_names_full, y_data_full,\\\n",
    "                                                        test_size=0.02, random_state=0)\n",
    "X_test_names, X_val_names, y_test, y_val = train_test_split(X_val_names, y_val,\\\n",
    "                                                            test_size=0.5,random_state=0)\n",
    "\n",
    "\n",
    "print ('Start training')\n",
    "# Training and validation inputs are fed from generators\n",
    "# Number of samples based on data_set size and adjusted to fit batch size\n",
    "history = model.fit_generator(generator(X_tr_names, y_tr, 64,preprocessing=proc_img),\\\n",
    "                              samples_per_epoch=len(X_tr_names),\n",
    "                              nb_epoch=5,\n",
    "                              validation_data=generator(X_val_names, y_val, 32, preprocessing=proc_img),\n",
    "                              nb_val_samples=len(y_val),  callbacks=[checkpoint, early_stop, TQDMNotebookCallback()], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.02466822401673972, 0.02466822401673972]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(generator(X_test_names,y_test,32, preprocessing=proc_img), val_samples=len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.057913975116748916,\n",
       "  0.045016651074239862,\n",
       "  0.036212845688239639,\n",
       "  0.029816109134606687,\n",
       "  0.024952148239233536],\n",
       " 'mean_squared_error': [0.057913975116748916,\n",
       "  0.045016651074239862,\n",
       "  0.036212845688239639,\n",
       "  0.029816109134606687,\n",
       "  0.024952148239233536],\n",
       " 'val_loss': [0.04746215378373176,\n",
       "  0.03799192775469059,\n",
       "  0.03152021486312151,\n",
       "  0.027574829382543593,\n",
       "  0.023121910576505221],\n",
       " 'val_mean_squared_error': [0.04746215378373176,\n",
       "  0.03799192775469059,\n",
       "  0.03152021486312151,\n",
       "  0.027574829382543593,\n",
       "  0.023121910576505221]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from car_models import nvidia_net\n",
    "\n",
    "model=nvidia_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed widget Javascript is the wrong version.\n",
      "The installed widget Javascript is the wrong version.\n",
      "/root/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training.py:1569: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n",
      "The installed widget Javascript is the wrong version.\n",
      "The installed widget Javascript is the wrong version.\n",
      "The installed widget Javascript is the wrong version.\n",
      "The installed widget Javascript is the wrong version.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import History,TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"model_gen5-nvidia2.h5\" , monitor='val_mean_squared_error', verbose=False,\n",
    "                          save_best_only=True, mode='min')\n",
    "early_stop = EarlyStopping(monitor='val_mean_squared_error',\\\n",
    "                           min_delta=0.001, patience=3,\n",
    "                            verbose=1, mode='min')\n",
    "\n",
    "# Compile, train and save\n",
    "#model.compile(optimizer=Adam(lr=FLAGS.learn_rate), loss='mse')\n",
    "\n",
    "\n",
    "model.compile(loss='mse', optimizer=Adam(lr=0.0001), metrics=['mean_squared_error'])\n",
    "\n",
    "\n",
    "#print ('Split data')\n",
    "#X_tr_names, X_val_names, y_tr, y_val = newRandomTestValidationSplit(image_names_full, y_data_full)\n",
    "\n",
    "X_tr_names, X_val_names, y_tr, y_val = train_test_split(image_names_full, y_data_full,\\\n",
    "                                                        test_size=0.02, random_state=0)\n",
    "X_test_names, X_val_names, y_test, y_val = train_test_split(X_val_names, y_val,\\\n",
    "                                                            test_size=0.5,random_state=0)\n",
    "\n",
    "\n",
    "print ('Start training')\n",
    "# Training and validation inputs are fed from generators\n",
    "# Number of samples based on data_set size and adjusted to fit batch size\n",
    "history = model.fit_generator(generator(X_tr_names, y_tr, 64),\\\n",
    "                              samples_per_epoch=len(X_tr_names),\n",
    "                              nb_epoch=5,\n",
    "                              validation_data=generator(X_val_names, y_val, 32),\n",
    "                              nb_val_samples=len(y_val),  callbacks=[checkpoint, early_stop, TQDMNotebookCallback()], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.028719550153861444, 0.028719550153861444]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(generator(X_test_names,y_test,32), val_samples=len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "cropping2d_2 (Cropping2D)        (None, 90, 320, 3)    0           cropping2d_input_2[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)                (None, 90, 320, 3)    0           cropping2d_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_34 (Convolution2D) (None, 86, 316, 24)   1824        lambda_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_20 (MaxPooling2D)   (None, 43, 158, 24)   0           convolution2d_34[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_35 (Convolution2D) (None, 39, 154, 36)   21636       maxpooling2d_20[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_21 (MaxPooling2D)   (None, 19, 77, 36)    0           convolution2d_35[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_36 (Convolution2D) (None, 15, 73, 48)    43248       maxpooling2d_21[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_22 (MaxPooling2D)   (None, 7, 36, 48)     0           convolution2d_36[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_37 (Convolution2D) (None, 7, 36, 64)     27712       maxpooling2d_22[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_38 (Convolution2D) (None, 7, 36, 64)     36928       convolution2d_37[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)             (None, 16128)         0           convolution2d_38[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dense_22 (Dense)                 (None, 1024)          16516096    flatten_14[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)             (None, 1024)          0           dense_22[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_23 (Dense)                 (None, 100)           102500      dropout_16[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)             (None, 100)           0           dense_23[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_24 (Dense)                 (None, 50)            5050        dropout_17[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)             (None, 50)            0           dense_24[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_25 (Dense)                 (None, 10)            510         dropout_18[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_26 (Dense)                 (None, 1)             11          dense_25[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 16,755,515\n",
      "Trainable params: 16,755,515\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carnd",
   "language": "python",
   "name": "carnd-term1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
